{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c0749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "import os\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3924ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(x_train, x_test, y_train, y_test, model):\n",
    "    model.fit(x_train, y_train)\n",
    "    print('Training Score:', model.score(x_train, y_train))\n",
    "    print('Training Root Mean Squared Error:', np.sqrt(mean_squared_error(y_train, model.predict(x_train))))\n",
    "    print('Test Score:', model.score(x_test, y_test))\n",
    "    print('Test Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, model.predict(x_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656a5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('log_transformed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54ed0d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('log_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "517a21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d48b351",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('Id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84a2060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.SalePrice\n",
    "x = df.drop('SalePrice', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c026932f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>LotFrontage_nan</th>\n",
       "      <th>MasVnrArea_nan</th>\n",
       "      <th>GarageYrBlt_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.110874</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.189655</td>\n",
       "      <td>9.042040</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>7.605392</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.044522</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.394449</td>\n",
       "      <td>9.169623</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>7.604894</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.110874</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.234107</td>\n",
       "      <td>9.328212</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>7.605392</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.262680</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.110874</td>\n",
       "      <td>9.164401</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>7.604396</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.110874</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.442651</td>\n",
       "      <td>9.565284</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>7.605392</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>4.110874</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.143135</td>\n",
       "      <td>8.976894</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>7.604894</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>3.044522</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.454347</td>\n",
       "      <td>9.486152</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>7.606387</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>4.262680</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.204693</td>\n",
       "      <td>9.109746</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>7.824446</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>7.606387</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>3.044522</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.234107</td>\n",
       "      <td>9.181735</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>7.606387</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>3.044522</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.330733</td>\n",
       "      <td>9.204121</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>7.605392</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  MSZoning  LotFrontage   LotArea    Street     Alley  \\\n",
       "0       4.110874  1.386294     4.189655  9.042040  0.693147  1.098612   \n",
       "1       3.044522  1.386294     4.394449  9.169623  0.693147  1.098612   \n",
       "2       4.110874  1.386294     4.234107  9.328212  0.693147  1.098612   \n",
       "3       4.262680  1.386294     4.110874  9.164401  0.693147  1.098612   \n",
       "4       4.110874  1.386294     4.442651  9.565284  0.693147  1.098612   \n",
       "...          ...       ...          ...       ...       ...       ...   \n",
       "1455    4.110874  1.386294     4.143135  8.976894  0.693147  1.098612   \n",
       "1456    3.044522  1.386294     4.454347  9.486152  0.693147  1.098612   \n",
       "1457    4.262680  1.386294     4.204693  9.109746  0.693147  1.098612   \n",
       "1458    3.044522  1.386294     4.234107  9.181735  0.693147  1.098612   \n",
       "1459    3.044522  1.386294     4.330733  9.204121  0.693147  1.098612   \n",
       "\n",
       "      LotShape  LandContour  Utilities  LotConfig  ...     Fence  MiscFeature  \\\n",
       "0     0.000000     0.693147   0.693147   0.000000  ...  1.609438     1.386294   \n",
       "1     0.000000     0.693147   0.693147   0.693147  ...  1.609438     1.386294   \n",
       "2     0.693147     0.693147   0.693147   0.000000  ...  1.609438     1.386294   \n",
       "3     0.693147     0.693147   0.693147   1.098612  ...  1.609438     1.386294   \n",
       "4     0.693147     0.693147   0.693147   0.693147  ...  1.609438     1.386294   \n",
       "...        ...          ...        ...        ...  ...       ...          ...   \n",
       "1455  0.000000     0.693147   0.693147   0.000000  ...  1.609438     1.386294   \n",
       "1456  0.000000     0.693147   0.693147   0.000000  ...  1.098612     1.386294   \n",
       "1457  0.000000     0.693147   0.693147   0.000000  ...  1.386294     0.693147   \n",
       "1458  0.000000     0.693147   0.693147   0.000000  ...  1.609438     1.386294   \n",
       "1459  0.000000     0.693147   0.693147   0.000000  ...  1.609438     1.386294   \n",
       "\n",
       "       MiscVal    MoSold    YrSold  SaleType  SaleCondition  LotFrontage_nan  \\\n",
       "0     0.000000  1.098612  7.605392  1.609438       1.609438              0.0   \n",
       "1     0.000000  1.791759  7.604894  1.609438       1.609438              0.0   \n",
       "2     0.000000  2.302585  7.605392  1.609438       1.609438              0.0   \n",
       "3     0.000000  1.098612  7.604396  1.609438       0.693147              0.0   \n",
       "4     0.000000  2.564949  7.605392  1.609438       1.609438              0.0   \n",
       "...        ...       ...       ...       ...            ...              ...   \n",
       "1455  0.000000  2.197225  7.604894  1.609438       1.609438              0.0   \n",
       "1456  0.000000  1.098612  7.606387  1.609438       1.609438              0.0   \n",
       "1457  7.824446  1.791759  7.606387  1.609438       1.609438              0.0   \n",
       "1458  0.000000  1.609438  7.606387  1.609438       1.609438              0.0   \n",
       "1459  0.000000  1.945910  7.605392  1.609438       1.609438              0.0   \n",
       "\n",
       "      MasVnrArea_nan  GarageYrBlt_nan  \n",
       "0                0.0              0.0  \n",
       "1                0.0              0.0  \n",
       "2                0.0              0.0  \n",
       "3                0.0              0.0  \n",
       "4                0.0              0.0  \n",
       "...              ...              ...  \n",
       "1455             0.0              0.0  \n",
       "1456             0.0              0.0  \n",
       "1457             0.0              0.0  \n",
       "1458             0.0              0.0  \n",
       "1459             0.0              0.0  \n",
       "\n",
       "[1460 rows x 82 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cd2688",
   "metadata": {},
   "source": [
    "#### With Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2122bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a83c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>LotFrontage_nan</th>\n",
       "      <th>MasVnrArea_nan</th>\n",
       "      <th>GarageYrBlt_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.044522</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.330733</td>\n",
       "      <td>9.218804</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>7.604894</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>4.262680</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.976734</td>\n",
       "      <td>9.970445</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>7.605890</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>4.510860</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>4.418841</td>\n",
       "      <td>8.873048</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>7.604894</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>3.044522</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.488636</td>\n",
       "      <td>9.482350</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>...</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>7.604396</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>3.044522</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.394449</td>\n",
       "      <td>9.169623</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>7.604396</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>3.044522</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.369448</td>\n",
       "      <td>9.224342</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>7.605890</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>3.044522</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4.394449</td>\n",
       "      <td>9.202409</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>7.606387</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>4.262680</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>4.110874</td>\n",
       "      <td>8.836810</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>7.604894</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>5.081404</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>7.427144</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>7.605392</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>3.433987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.795791</td>\n",
       "      <td>9.798183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>6.329721</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>7.605392</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1095 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  MSZoning  LotFrontage   LotArea    Street     Alley  \\\n",
       "6       3.044522  1.386294     4.330733  9.218804  0.693147  1.098612   \n",
       "807     4.262680  1.386294     4.976734  9.970445  0.693147  1.098612   \n",
       "955     4.510860  1.098612     4.418841  8.873048  0.693147  1.098612   \n",
       "1040    3.044522  1.386294     4.488636  9.482350  0.693147  1.098612   \n",
       "701     3.044522  1.386294     4.394449  9.169623  0.693147  1.098612   \n",
       "...          ...       ...          ...       ...       ...       ...   \n",
       "715     3.044522  1.386294     4.369448  9.224342  0.693147  1.098612   \n",
       "905     3.044522  1.386294     4.394449  9.202409  0.693147  1.098612   \n",
       "1096    4.262680  0.693147     4.110874  8.836810  0.693147  1.098612   \n",
       "235     5.081404  0.693147     3.091042  7.427144  0.693147  1.098612   \n",
       "1061    3.433987  0.000000     4.795791  9.798183  0.000000  1.098612   \n",
       "\n",
       "      LotShape  LandContour  Utilities  LotConfig  ...     Fence  MiscFeature  \\\n",
       "6     0.000000     0.693147   0.693147   0.000000  ...  1.609438     1.386294   \n",
       "807   0.000000     0.693147   0.693147   0.000000  ...  1.609438     1.386294   \n",
       "955   0.693147     1.386294   0.693147   0.000000  ...  1.609438     1.386294   \n",
       "1040  0.000000     0.693147   0.693147   1.098612  ...  1.386294     1.386294   \n",
       "701   0.000000     0.693147   0.693147   0.000000  ...  1.609438     1.386294   \n",
       "...        ...          ...        ...        ...  ...       ...          ...   \n",
       "715   0.000000     0.693147   0.693147   0.000000  ...  1.098612     1.386294   \n",
       "905   0.000000     0.693147   0.693147   0.000000  ...  1.098612     1.386294   \n",
       "1096  0.000000     0.693147   0.693147   0.000000  ...  1.609438     1.386294   \n",
       "235   0.000000     0.693147   0.693147   0.000000  ...  1.609438     1.386294   \n",
       "1061  0.000000     1.098612   0.693147   0.000000  ...  1.609438     0.693147   \n",
       "\n",
       "       MiscVal    MoSold    YrSold  SaleType  SaleCondition  LotFrontage_nan  \\\n",
       "6     0.000000  2.197225  7.604894  1.609438       1.609438              0.0   \n",
       "807   0.000000  1.791759  7.605890  1.609438       1.609438              0.0   \n",
       "955   0.000000  2.197225  7.604894  1.609438       1.609438              0.0   \n",
       "1040  0.000000  0.693147  7.604396  1.609438       1.609438              0.0   \n",
       "701   0.000000  2.079442  7.604396  1.386294       1.609438              0.0   \n",
       "...        ...       ...       ...       ...            ...              ...   \n",
       "715   0.000000  2.197225  7.605890  1.609438       1.609438              0.0   \n",
       "905   0.000000  1.098612  7.606387  1.609438       1.609438              0.0   \n",
       "1096  0.000000  1.386294  7.604894  1.609438       1.609438              0.0   \n",
       "235   0.000000  2.197225  7.605392  1.609438       1.609438              0.0   \n",
       "1061  6.329721  2.197225  7.605392  0.693147       1.609438              0.0   \n",
       "\n",
       "      MasVnrArea_nan  GarageYrBlt_nan  \n",
       "6                0.0         0.000000  \n",
       "807              0.0         0.000000  \n",
       "955              0.0         0.000000  \n",
       "1040             0.0         0.000000  \n",
       "701              0.0         0.000000  \n",
       "...              ...              ...  \n",
       "715              0.0         0.000000  \n",
       "905              0.0         0.000000  \n",
       "1096             0.0         0.693147  \n",
       "235              0.0         0.000000  \n",
       "1061             0.0         0.000000  \n",
       "\n",
       "[1095 rows x 82 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d476768",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d63b2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9045251664029565\n",
      "Training Root Mean Squared Error: 0.12218198482489515\n",
      "Test Score: 0.8955216821201339\n",
      "Test Root Mean Squared Error: 0.1323265167392523\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e8e2509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.51571417, 12.07070202, 12.01138433, ..., 11.66955498,\n",
       "       11.35786901, 11.28934785])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50a97207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.69076464, 11.9813162 , 12.08545906, ..., 12.0164575 ,\n",
       "       11.77273369, 12.35029445])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dcde928",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f871f090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8876422283158587\n",
      "Training Root Mean Squared Error: 0.1325452838739856\n",
      "Test Score: 0.8907003952616598\n",
      "Test Root Mean Squared Error: 0.13534527238100794\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62805a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv1 = SVR(kernel='poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b442c59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9127399075558782\n",
      "Training Root Mean Squared Error: 0.11680745327255906\n",
      "Test Score: 0.9072201500483683\n",
      "Test Root Mean Squared Error: 0.12469832722550982\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, sv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a97fb947",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv2 = SVR(gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d52da3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9329052766711605\n",
      "Training Root Mean Squared Error: 0.10242521966328126\n",
      "Test Score: 0.8684144287816362\n",
      "Test Root Mean Squared Error: 0.14850392412979999\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, sv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "baa19e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "153103b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(min_samples_split=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62144d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9223804381934795\n",
      "Training Root Mean Squared Error: 0.1101661859803049\n",
      "Test Score: 0.8060833594226205\n",
      "Test Root Mean Squared Error: 0.18027742000151145\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3d2011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6810446",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=47, min_samples_split=25, random_state=2) #47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82e88097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9283541184588486\n",
      "Training Root Mean Squared Error: 0.10584207273344566\n",
      "Test Score: 0.8708910845502993\n",
      "Test Root Mean Squared Error: 0.1470997420655804\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "400342b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce9f45be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(eta0=0.0039, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b44b556b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8460191494133678\n",
      "Training Root Mean Squared Error: 0.1551658261222209\n",
      "Test Score: 0.8517128005751061\n",
      "Test Root Mean Squared Error: 0.1576469693943467\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af9fe922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4a61439",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = XGBRegressor(max_depth=2, learning_rate=0.330000012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fce4f012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9600528905283058\n",
      "Training Root Mean Squared Error: 0.07903244189294235\n",
      "Test Score: 0.9075379430687305\n",
      "Test Root Mean Squared Error: 0.12448458334495083\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a980f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c383788",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae855af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=75, kernel_initializer = 'he_uniform', activation='relu', input_dim=82))\n",
    "model.add(Dense(units=15, kernel_initializer = 'he_uniform', activation='relu'))\n",
    "model.add(Dense(units=15, kernel_initializer = 'he_uniform', activation='relu'))\n",
    "model.add(Dense(units=15, kernel_initializer = 'he_uniform', activation='relu'))\n",
    "model.add(Dense(units=1, kernel_initializer = 'glorot_uniform', activation = 'linear' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61860f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss= 'MeanSquaredError')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b531ec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "110/110 [==============================] - 1s 3ms/step - loss: 30.7553 - val_loss: 0.3905\n",
      "Epoch 2/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2198 - val_loss: 0.1579\n",
      "Epoch 3/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.1064 - val_loss: 0.1015\n",
      "Epoch 4/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0738 - val_loss: 0.0872\n",
      "Epoch 5/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0618 - val_loss: 0.0646\n",
      "Epoch 6/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0540 - val_loss: 0.0682\n",
      "Epoch 7/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0489 - val_loss: 0.0525\n",
      "Epoch 8/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0471 - val_loss: 0.0501\n",
      "Epoch 9/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0419 - val_loss: 0.0460\n",
      "Epoch 10/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0401 - val_loss: 0.0437\n",
      "Epoch 11/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0387 - val_loss: 0.0589\n",
      "Epoch 12/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0376 - val_loss: 0.0545\n",
      "Epoch 13/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 0.0401\n",
      "Epoch 14/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0353 - val_loss: 0.0391\n",
      "Epoch 15/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0381\n",
      "Epoch 16/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 0.0387\n",
      "Epoch 17/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0336\n",
      "Epoch 18/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 0.0380\n",
      "Epoch 19/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0319 - val_loss: 0.0331\n",
      "Epoch 20/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0324 - val_loss: 0.0404\n",
      "Epoch 21/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 0.0323\n",
      "Epoch 22/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0513\n",
      "Epoch 23/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0413\n",
      "Epoch 24/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0295 - val_loss: 0.0339\n",
      "Epoch 25/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0333\n",
      "Epoch 26/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0333\n",
      "Epoch 27/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0310\n",
      "Epoch 28/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0277\n",
      "Epoch 29/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0493\n",
      "Epoch 30/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0275\n",
      "Epoch 31/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.0365\n",
      "Epoch 32/200\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.026 - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0344\n",
      "Epoch 33/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.0272\n",
      "Epoch 34/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0267\n",
      "Epoch 35/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0366\n",
      "Epoch 36/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0266\n",
      "Epoch 37/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0307\n",
      "Epoch 38/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0200 - val_loss: 0.0272\n",
      "Epoch 39/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0561\n",
      "Epoch 40/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 0.0252\n",
      "Epoch 41/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0287\n",
      "Epoch 42/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0270\n",
      "Epoch 43/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0283\n",
      "Epoch 44/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0326\n",
      "Epoch 45/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0279\n",
      "Epoch 46/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0297\n",
      "Epoch 47/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0333\n",
      "Epoch 48/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.0272\n",
      "Epoch 49/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0262\n",
      "Epoch 50/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0352\n",
      "Epoch 51/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0455\n",
      "Epoch 52/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0265\n",
      "Epoch 53/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0340\n",
      "Epoch 54/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0252\n",
      "Epoch 55/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0251\n",
      "Epoch 56/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0356\n",
      "Epoch 57/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0277\n",
      "Epoch 58/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0316\n",
      "Epoch 59/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0381\n",
      "Epoch 60/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0242\n",
      "Epoch 61/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0258\n",
      "Epoch 62/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0219\n",
      "Epoch 63/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0268\n",
      "Epoch 64/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0932\n",
      "Epoch 65/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0238\n",
      "Epoch 66/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0235\n",
      "Epoch 67/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0364\n",
      "Epoch 68/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0544\n",
      "Epoch 69/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0276\n",
      "Epoch 70/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0298\n",
      "Epoch 71/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.0234\n",
      "Epoch 72/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0306\n",
      "Epoch 73/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0379\n",
      "Epoch 74/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0288\n",
      "Epoch 75/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0241\n",
      "Epoch 76/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0257\n",
      "Epoch 77/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0224\n",
      "Epoch 78/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0253\n",
      "Epoch 79/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0399\n",
      "Epoch 80/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0469\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.0502\n",
      "Epoch 82/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0274\n",
      "Epoch 83/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0331\n",
      "Epoch 84/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0390\n",
      "Epoch 85/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0229\n",
      "Epoch 86/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0273\n",
      "Epoch 87/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0389\n",
      "Epoch 88/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0341\n",
      "Epoch 89/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0587\n",
      "Epoch 90/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0288\n",
      "Epoch 91/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0479\n",
      "Epoch 92/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0264\n",
      "Epoch 93/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.0249\n",
      "Epoch 94/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0243\n",
      "Epoch 95/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0206\n",
      "Epoch 96/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0261\n",
      "Epoch 97/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0323\n",
      "Epoch 98/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0421\n",
      "Epoch 99/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0316\n",
      "Epoch 100/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.0303\n",
      "Epoch 101/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0224\n",
      "Epoch 102/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0233\n",
      "Epoch 103/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0279\n",
      "Epoch 104/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0299\n",
      "Epoch 105/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0310\n",
      "Epoch 106/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0233\n",
      "Epoch 107/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0571\n",
      "Epoch 108/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0242\n",
      "Epoch 109/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0267\n",
      "Epoch 110/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0424\n",
      "Epoch 111/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0256\n",
      "Epoch 112/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0258\n",
      "Epoch 113/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0307\n",
      "Epoch 114/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0270\n",
      "Epoch 115/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0269\n",
      "Epoch 116/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0324\n",
      "Epoch 117/200\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.016 - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0289\n",
      "Epoch 118/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0210\n",
      "Epoch 119/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0256\n",
      "Epoch 120/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0256\n",
      "Epoch 121/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0347\n",
      "Epoch 122/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0256\n",
      "Epoch 123/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0234\n",
      "Epoch 124/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0265\n",
      "Epoch 125/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0262\n",
      "Epoch 126/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0290\n",
      "Epoch 127/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0301\n",
      "Epoch 128/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0232\n",
      "Epoch 129/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0464\n",
      "Epoch 130/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0225\n",
      "Epoch 131/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.0271\n",
      "Epoch 132/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0238\n",
      "Epoch 133/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.0251\n",
      "Epoch 134/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.0239\n",
      "Epoch 135/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0131 - val_loss: 0.0260\n",
      "Epoch 136/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.0293\n",
      "Epoch 137/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0244\n",
      "Epoch 138/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0314\n",
      "Epoch 139/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0239\n",
      "Epoch 140/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0264\n",
      "Epoch 141/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0506\n",
      "Epoch 142/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0671\n",
      "Epoch 143/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0363\n",
      "Epoch 144/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0266\n",
      "Epoch 145/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0266\n",
      "Epoch 146/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.0247\n",
      "Epoch 147/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0233\n",
      "Epoch 148/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.0263\n",
      "Epoch 149/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0454\n",
      "Epoch 150/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.0302\n",
      "Epoch 151/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0270\n",
      "Epoch 152/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0291\n",
      "Epoch 153/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0685\n",
      "Epoch 154/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0353\n",
      "Epoch 155/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0401\n",
      "Epoch 156/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0234\n",
      "Epoch 157/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0328\n",
      "Epoch 158/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0259\n",
      "Epoch 159/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0305\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0239\n",
      "Epoch 161/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0263\n",
      "Epoch 162/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0249\n",
      "Epoch 163/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0251\n",
      "Epoch 164/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.0241\n",
      "Epoch 165/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0247\n",
      "Epoch 166/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0261\n",
      "Epoch 167/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.0246\n",
      "Epoch 168/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0339\n",
      "Epoch 169/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0317\n",
      "Epoch 170/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0355\n",
      "Epoch 171/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0615\n",
      "Epoch 172/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0294\n",
      "Epoch 173/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0357\n",
      "Epoch 174/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0246\n",
      "Epoch 175/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0270\n",
      "Epoch 176/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0282\n",
      "Epoch 177/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0235\n",
      "Epoch 178/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0284\n",
      "Epoch 179/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0242\n",
      "Epoch 180/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.0530\n",
      "Epoch 181/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0675\n",
      "Epoch 182/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.0270\n",
      "Epoch 183/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.0290\n",
      "Epoch 184/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0222\n",
      "Epoch 185/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.0252\n",
      "Epoch 186/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.0319\n",
      "Epoch 187/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.0221\n",
      "Epoch 188/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.0255\n",
      "Epoch 189/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.0238\n",
      "Epoch 190/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 0.0288\n",
      "Epoch 191/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0227\n",
      "Epoch 192/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0281\n",
      "Epoch 193/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.0225\n",
      "Epoch 194/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 0.0286\n",
      "Epoch 195/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0261\n",
      "Epoch 196/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.0208\n",
      "Epoch 197/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0127 - val_loss: 0.0219\n",
      "Epoch 198/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.0227\n",
      "Epoch 199/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0277\n",
      "Epoch 200/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0340\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x= x_train, y= y_train, validation_data=(x_test, y_test), batch_size=10, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2eea4ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c3ed6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'val_loss'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4198df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(range(200), history.history['loss'], label = 'Training Loss')\n",
    "plt.plot(range(200), history.history['val_loss'], label = 'Testing Loss')\n",
    "plt.title('Training Vs Testing Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "419e6968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010426663793623447"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e87a1162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020551754161715508"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcacb73",
   "metadata": {},
   "source": [
    "#### Without Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f817ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(data):\n",
    "    q1 = data.quantile(0.25)\n",
    "    q3 = data.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    low_lim = q1 - 1.5 * iqr\n",
    "    upper_lim = q3 + 1.5 * iqr\n",
    "    data = np.where(data > upper_lim, upper_lim,\n",
    "                   np.where(data < low_lim, low_lim, data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "feb8544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = outliers(x)\n",
    "y1 = outliers(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a2d0eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a3c5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b53124f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8991403501489558\n",
      "Training Root Mean Squared Error: 0.12165454897159117\n",
      "Test Score: 0.9146568954181985\n",
      "Test Root Mean Squared Error: 0.11585824777855513\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e9ed496b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.70164095e+10,  2.29211793e+09, -4.32212340e+09, ...,\n",
       "       -7.75278576e+09, -2.63754135e+10, -3.53315474e+09])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13396294",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = outliers(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7443b527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.68951416, 11.97415161, 12.09857178, ..., 12.14009094,\n",
       "       11.70359802, 12.34599304])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "02e242e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8816248759494543\n",
      "Training Root Mean Squared Error: 0.13179527737601943\n",
      "Test Score: 0.8972498581766267\n",
      "Test Root Mean Squared Error: 0.1271258741608074\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4957a206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8995142626791384\n",
      "Training Root Mean Squared Error: 0.12142883731349709\n",
      "Test Score: 0.9137190282806666\n",
      "Test Root Mean Squared Error: 0.11649311305292441\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, sv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87505dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9150928764023294\n",
      "Training Root Mean Squared Error: 0.11161991543296698\n",
      "Test Score: 0.9019797508431013\n",
      "Test Root Mean Squared Error: 0.1241654132656294\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, sv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "81bff73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.917685448275582\n",
      "Training Root Mean Squared Error: 0.10990259160017923\n",
      "Test Score: 0.7906204359303804\n",
      "Test Root Mean Squared Error: 0.18147210752530787\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b56501e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9324697298564326\n",
      "Training Root Mean Squared Error: 0.09954486647919042\n",
      "Test Score: 0.8818916615426837\n",
      "Test Root Mean Squared Error: 0.13629597077108702\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aed010fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8425481800672736\n",
      "Training Root Mean Squared Error: 0.1519999577818096\n",
      "Test Score: 0.8648090966888475\n",
      "Test Root Mean Squared Error: 0.14581979127351133\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6f9f2561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9576686133261973\n",
      "Training Root Mean Squared Error: 0.07881354683837184\n",
      "Test Score: 0.9027184133326197\n",
      "Test Root Mean Squared Error: 0.12369668472464858\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "43cb42f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5fb9a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=3, leaf_size=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "feafce88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8544050254150398\n",
      "Training Root Mean Squared Error: 0.14616480696910786\n",
      "Test Score: 0.7554930692798668\n",
      "Test Root Mean Squared Error: 0.19610484534048256\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dc091bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ARDRegression, HuberRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6efc0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ard = ARDRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "490c3b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8969351364915865\n",
      "Training Root Mean Squared Error: 0.12297729641810178\n",
      "Test Score: 0.9148724403921455\n",
      "Test Root Mean Squared Error: 0.11571184783809142\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, ard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec28009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "huber = HuberRegressor(max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "26cd4938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.89549133606162\n",
      "Training Root Mean Squared Error: 0.12383567413992091\n",
      "Test Score: 0.9168161056073976\n",
      "Test Root Mean Squared Error: 0.11438323144745521\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, huber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e2a0a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b1f5145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=75, kernel_initializer = 'he_uniform', activation='relu', input_dim=82)) #75\n",
    "model.add(Dense(units=15, kernel_initializer = 'he_uniform', activation='relu'))\n",
    "model.add(Dense(units=15, kernel_initializer = 'he_uniform', activation='relu'))\n",
    "model.add(Dense(units=15, kernel_initializer = 'he_uniform', activation='relu'))\n",
    "model.add(Dense(units=1, kernel_initializer = 'glorot_uniform', activation = 'linear' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d4cc23da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 75)                6225      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 15)                1140      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 15)                240       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 15)                240       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,861\n",
      "Trainable params: 7,861\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eb46e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss= 'MeanSquaredError')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "768078bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = keras.callbacks.ModelCheckpoint(filepath=os.getcwd(), monitor='val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d788988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 86/110 [======================>.......] - ETA: 0s - loss: 30.4912INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 24.0412 - val_loss: 0.3878\n",
      "Epoch 2/200\n",
      " 71/110 [==================>...........] - ETA: 0s - loss: 0.2510INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.1988 - val_loss: 0.0813\n",
      "Epoch 3/200\n",
      " 78/110 [====================>.........] - ETA: 0s - loss: 0.0642INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.0600 - val_loss: 0.0408\n",
      "Epoch 4/200\n",
      " 73/110 [==================>...........] - ETA: 0s - loss: 0.0395INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.0415 - val_loss: 0.0341\n",
      "Epoch 5/200\n",
      " 74/110 [===================>..........] - ETA: 0s - loss: 0.0406INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0376 - val_loss: 0.0292\n",
      "Epoch 6/200\n",
      " 79/110 [====================>.........] - ETA: 0s - loss: 0.0319INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.0330 - val_loss: 0.0272\n",
      "Epoch 7/200\n",
      " 78/110 [====================>.........] - ETA: 0s - loss: 0.0367INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.0340 - val_loss: 0.0264\n",
      "Epoch 8/200\n",
      " 76/110 [===================>..........] - ETA: 0s - loss: 0.0306INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0304 - val_loss: 0.0240\n",
      "Epoch 9/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.0264\n",
      "Epoch 10/200\n",
      " 73/110 [==================>...........] - ETA: 0s - loss: 0.0243INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0274 - val_loss: 0.0227\n",
      "Epoch 11/200\n",
      " 78/110 [====================>.........] - ETA: 0s - loss: 0.0287INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.0272 - val_loss: 0.0211\n",
      "Epoch 12/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0218\n",
      "Epoch 13/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0262\n",
      "Epoch 14/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0237\n",
      "Epoch 15/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0225\n",
      "Epoch 16/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.0355\n",
      "Epoch 17/200\n",
      " 99/110 [==========================>...] - ETA: 0s - loss: 0.0250INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.0247 - val_loss: 0.0203\n",
      "Epoch 18/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0278\n",
      "Epoch 19/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0401\n",
      "Epoch 20/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0216\n",
      "Epoch 21/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0228\n",
      "Epoch 22/200\n",
      "102/110 [==========================>...] - ETA: 0s - loss: 0.0221INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.0218 - val_loss: 0.0178\n",
      "Epoch 23/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0247\n",
      "Epoch 24/200\n",
      " 73/110 [==================>...........] - ETA: 0s - loss: 0.0242INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 0.0258 - val_loss: 0.0174\n",
      "Epoch 25/200\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0255INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0255 - val_loss: 0.0170\n",
      "Epoch 26/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0170\n",
      "Epoch 27/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0200\n",
      "Epoch 28/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.0172\n",
      "Epoch 29/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0299\n",
      "Epoch 30/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0221\n",
      "Epoch 31/200\n",
      " 75/110 [===================>..........] - ETA: 0s - loss: 0.0248INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.0237 - val_loss: 0.0162\n",
      "Epoch 32/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0171\n",
      "Epoch 33/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 0.0231\n",
      "Epoch 34/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0229\n",
      "Epoch 35/200\n",
      " 75/110 [===================>..........] - ETA: 0s - loss: 0.0256INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.0243 - val_loss: 0.0158\n",
      "Epoch 36/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0220\n",
      "Epoch 37/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0161\n",
      "Epoch 38/200\n",
      " 78/110 [====================>.........] - ETA: 0s - loss: 0.0250INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0232 - val_loss: 0.0153\n",
      "Epoch 39/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0245\n",
      "Epoch 40/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0203\n",
      "Epoch 41/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0204\n",
      "Epoch 42/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0162\n",
      "Epoch 43/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0155\n",
      "Epoch 44/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0193\n",
      "Epoch 45/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0297\n",
      "Epoch 46/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0197\n",
      "Epoch 47/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0253\n",
      "Epoch 48/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0179\n",
      "Epoch 49/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0305\n",
      "Epoch 50/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0334\n",
      "Epoch 51/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0230\n",
      "Epoch 52/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0481\n",
      "Epoch 53/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0198\n",
      "Epoch 54/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0326\n",
      "Epoch 55/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0174\n",
      "Epoch 56/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0302\n",
      "Epoch 57/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0391 - val_loss: 0.0225\n",
      "Epoch 58/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0188\n",
      "Epoch 59/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0184\n",
      "Epoch 60/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0166\n",
      "Epoch 61/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0333\n",
      "Epoch 62/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0157\n",
      "Epoch 63/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0314 - val_loss: 0.0357\n",
      "Epoch 64/200\n",
      " 78/110 [====================>.........] - ETA: 0s - loss: 0.0206INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0214 - val_loss: 0.0143\n",
      "Epoch 65/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0573\n",
      "Epoch 66/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0407\n",
      "Epoch 67/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0152\n",
      "Epoch 68/200\n",
      " 75/110 [===================>..........] - ETA: 0s - loss: 0.0231INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.0228 - val_loss: 0.0142\n",
      "Epoch 69/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.0255\n",
      "Epoch 70/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0415\n",
      "Epoch 71/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0593\n",
      "Epoch 72/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.0154\n",
      "Epoch 73/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0185\n",
      "Epoch 74/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0395\n",
      "Epoch 75/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0181\n",
      "Epoch 76/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0319\n",
      "Epoch 77/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0143\n",
      "Epoch 78/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0163\n",
      "Epoch 79/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0143\n",
      "Epoch 80/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.0162\n",
      "Epoch 81/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0170\n",
      "Epoch 82/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0167\n",
      "Epoch 83/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0202\n",
      "Epoch 84/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0154\n",
      "Epoch 85/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0289\n",
      "Epoch 86/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0268\n",
      "Epoch 87/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0146\n",
      "Epoch 88/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0182\n",
      "Epoch 89/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0156\n",
      "Epoch 90/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0232\n",
      "Epoch 91/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0190\n",
      "Epoch 92/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0181\n",
      "Epoch 93/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0218\n",
      "Epoch 94/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0148\n",
      "Epoch 95/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0171\n",
      "Epoch 96/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.0203\n",
      "Epoch 97/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0262\n",
      "Epoch 98/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0333 - val_loss: 0.0152\n",
      "Epoch 99/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0168\n",
      "Epoch 100/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0156\n",
      "Epoch 101/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0160\n",
      "Epoch 102/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0161\n",
      "Epoch 103/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0186\n",
      "Epoch 104/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.0185\n",
      "Epoch 105/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0157\n",
      "Epoch 106/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0151\n",
      "Epoch 107/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0159\n",
      "Epoch 108/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0146\n",
      "Epoch 109/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0314\n",
      "Epoch 110/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0162\n",
      "Epoch 111/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0232\n",
      "Epoch 112/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0150\n",
      "Epoch 113/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0142\n",
      "Epoch 114/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0302\n",
      "Epoch 115/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0207\n",
      "Epoch 116/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0177\n",
      "Epoch 117/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0361\n",
      "Epoch 118/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0216\n",
      "Epoch 119/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.0380\n",
      "Epoch 120/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0149\n",
      "Epoch 121/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0278\n",
      "Epoch 122/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0322\n",
      "Epoch 123/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0150\n",
      "Epoch 124/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0149\n",
      "Epoch 125/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0339\n",
      "Epoch 126/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0436\n",
      "Epoch 127/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0195\n",
      "Epoch 128/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0145\n",
      "Epoch 129/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.0317\n",
      "Epoch 130/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0199 - val_loss: 0.0205\n",
      "Epoch 131/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0170\n",
      "Epoch 132/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.0183\n",
      "Epoch 133/200\n",
      "108/110 [============================>.] - ETA: 0s - loss: 0.0159INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.0159 - val_loss: 0.0139\n",
      "Epoch 134/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0264\n",
      "Epoch 135/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0192\n",
      "Epoch 136/200\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.018 - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0222\n",
      "Epoch 137/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.0195\n",
      "Epoch 138/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0972\n",
      "Epoch 139/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0146\n",
      "Epoch 140/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.0140\n",
      "Epoch 141/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.0164\n",
      "Epoch 142/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0175\n",
      "Epoch 143/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0144\n",
      "Epoch 144/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.0152\n",
      "Epoch 145/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0313\n",
      "Epoch 146/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.0152\n",
      "Epoch 147/200\n",
      " 79/110 [====================>.........] - ETA: 0s - loss: 0.0184INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 0.0180 - val_loss: 0.0137\n",
      "Epoch 148/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0159\n",
      "Epoch 149/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0247\n",
      "Epoch 150/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0170 - val_loss: 0.0223\n",
      "Epoch 151/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0145\n",
      "Epoch 152/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.0143\n",
      "Epoch 153/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0273\n",
      "Epoch 154/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0273\n",
      "Epoch 155/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.0143\n",
      "Epoch 156/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0149\n",
      "Epoch 157/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0184\n",
      "Epoch 158/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0156\n",
      "Epoch 159/200\n",
      " 81/110 [=====================>........] - ETA: 0s - loss: 0.0178INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.0176 - val_loss: 0.0136\n",
      "Epoch 160/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.0167\n",
      "Epoch 161/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 162/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0194\n",
      "Epoch 163/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0150\n",
      "Epoch 164/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.0142\n",
      "Epoch 165/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.0158\n",
      "Epoch 166/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.0175\n",
      "Epoch 167/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.0140\n",
      "Epoch 168/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.0147\n",
      "Epoch 169/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.0215\n",
      "Epoch 170/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0153\n",
      "Epoch 171/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0238\n",
      "Epoch 172/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0150\n",
      "Epoch 173/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0177\n",
      "Epoch 174/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0292\n",
      "Epoch 175/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.0143\n",
      "Epoch 176/200\n",
      " 83/110 [=====================>........] - ETA: 0s - loss: 0.0158INFO:tensorflow:Assets written to: C:\\Users\\rishi\\Desktop\\Projects\\kaggle\\house-prices-advanced-regression-techniques\\assets\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.0162 - val_loss: 0.0132\n",
      "Epoch 177/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.0150\n",
      "Epoch 178/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0215\n",
      "Epoch 179/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0145\n",
      "Epoch 180/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0198\n",
      "Epoch 181/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.0327\n",
      "Epoch 182/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0275\n",
      "Epoch 183/200\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.0177 - val_loss: 0.0187\n",
      "Epoch 184/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0228\n",
      "Epoch 185/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0140\n",
      "Epoch 186/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0312\n",
      "Epoch 187/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0155\n",
      "Epoch 188/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0152\n",
      "Epoch 189/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0167\n",
      "Epoch 190/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0217\n",
      "Epoch 191/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0140\n",
      "Epoch 192/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.0204\n",
      "Epoch 193/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.0136\n",
      "Epoch 194/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0211\n",
      "Epoch 195/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0143\n",
      "Epoch 196/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.0159\n",
      "Epoch 197/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.0139\n",
      "Epoch 198/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0151\n",
      "Epoch 199/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0236\n",
      "Epoch 200/200\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0155\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x= x_train, y= y_train, validation_data=(x_test, y_test), batch_size=10, epochs=200, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8b3a2283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1268857754044952"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(0.0161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5d8b0e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11224972160321824"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(0.0126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a5f2b8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x22933763250>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "36b66679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013905748706262431"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train, model.predict(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "65009895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11792263865035599"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(0.013905748706262431)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c6dddc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012645833845005778"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, model.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fe40e1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11245369644883078"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(0.012645833845005778)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "650a828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dde5cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = [('lr', LinearRegression()),\n",
    "            ('ard', ARDRegression()),\n",
    "            ('huber', HuberRegressor(max_iter=2000)),\n",
    "            ('svr', SVR(kernel='poly')),\n",
    "            ('xgb', XGBRegressor(max_depth=2, learning_rate=0.330000012))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0a6b6bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = VotingRegressor(estimators=estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c0743487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9199692529072652\n",
      "Training Root Mean Squared Error: 0.1083672519632374\n",
      "Test Score: 0.9215409159997086\n",
      "Test Root Mean Squared Error: 0.11108728536046282\n"
     ]
    }
   ],
   "source": [
    "score(x_train, x_test, y_train, y_test, voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "304a51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_score = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ffba4917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.534462 ],\n",
       "       [12.265187 ],\n",
       "       [12.046413 ],\n",
       "       ...,\n",
       "       [12.138509 ],\n",
       "       [11.4126835],\n",
       "       [12.407476 ]], dtype=float32)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "885d9671",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_score = np.e**ann_score -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "687cda2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[102175.98],\n",
       "       [212178.28],\n",
       "       [170485.78],\n",
       "       ...,\n",
       "       [186932.55],\n",
       "       [ 90460.82],\n",
       "       [244622.61]], dtype=float32)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4e425f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 1)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "48dc1f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_submission1 = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5e16eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_submission1['SalePrice'] = ann_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "da5fe483",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_submission1.to_csv('ann_submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0c84838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_score = voting.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9e67c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_score = np.e**voting_score -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "81ba70d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([122144.44647545, 159330.74408177, 183891.87808583, ...,\n",
       "       181600.239007  , 119287.64900199, 220722.24870258])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8369e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_submission1 = ann_submission1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a5e86d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_submission1.SalePrice = voting_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "11142487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>122144.446475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>159330.744082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>183891.878086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>195184.301150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>183071.961403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>83645.578162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>84857.003023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>181600.239007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>119287.649002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>220722.248703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  122144.446475\n",
       "1     1462  159330.744082\n",
       "2     1463  183891.878086\n",
       "3     1464  195184.301150\n",
       "4     1465  183071.961403\n",
       "...    ...            ...\n",
       "1454  2915   83645.578162\n",
       "1455  2916   84857.003023\n",
       "1456  2917  181600.239007\n",
       "1457  2918  119287.649002\n",
       "1458  2919  220722.248703\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_submission1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "52cad31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_submission1.to_csv('voting_submission1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5833a0be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
